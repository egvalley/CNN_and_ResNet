{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish CNNs from scratch\n",
    "The cross-entropy loss is commonly used in CNNs. This concept comes from Information Theory. The cross-entropy from $P$ to $Q$,denoted $H(P,Q) \\overset{def}{=} \\sum_{j} -P(j) \\log Q(j)$, is the expected surprisal of an observer with subjective probabilities $Q$  upon seeing data that was actually generated according to probabilities  $P$. The lowest possible cross-entropy is achieved when $P=Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from layer import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "\n",
    "    def softmax(self):\n",
    "        shifted_logits =  self.input- np.max(self.input)\n",
    "        tmp = np.exp(shifted_logits)\n",
    "        self.output = tmp / np.sum(tmp) \n",
    "        return self.output\n",
    "\n",
    "    def cross_entropy(self,y_true:list):\n",
    "        loss = -np.sum(y_true*np.log(self.output+1e-15))\n",
    "        return np.maximum(loss,0)\n",
    "\n",
    "\n",
    "    def forward(self,input):\n",
    "        self.input=input\n",
    "        #print(self.input)\n",
    "        return self.softmax()\n",
    "    \n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return self.output-output_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supplementary information from Prof. Mu Li  \n",
    "Since the softmax function and the corresponding cross-entropy loss are so common, it is worth understanding a bit better how they are computed. Here, we only consider the cross-entropy loss of consistent situations.  \n",
    "$$\n",
    "l(Y,\\hat Y) = - \\sum_{i=1}^{n} y_{i} \\log \\frac{exp(o_{i})}{\\sum_{k=1}^{n} exp(o_{k})}  \\\\\n",
    "\n",
    "=\\sum_{i=1}^{n} y_{i} \\log \\sum_{k=1}^{n} exp(o_{k}) -\\sum_{i=1}^{n} y_{i} o_{i}  \\\\\n",
    "\n",
    "=\\log \\sum_{k=1}^{n} exp(o_{k}) - \\sum_{i=1}^{n} y_{i} o_{i} \n",
    "$$  \n",
    "To understand a bit better what is going on, consider the derivative with respect to any logit $o_{j}$.\n",
    "$$\n",
    "\\partial_{o_{j}} l(Y,\\hat Y) = \\frac{exp(o_{i})}{\\sum_{k=1}^{n} exp(o_{k})}-y_{i}=softmax(o_{j}) -y_{j}\n",
    "$$  \n",
    "In other words, the derivative is the difference between the probability assigned by our model, as expressed by the softmax operation, and what actually happened, as expressed by elements in the one-hot label vector. In this sense, it is very similar to what we saw in regression, where the gradient was the difference between the observation $y$ and estimate $\\hat y$. This is not a coincidence. In any exponential family model, the gradients of the log-likelihood are given by precisely this term. This fact makes computing gradients easy in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
