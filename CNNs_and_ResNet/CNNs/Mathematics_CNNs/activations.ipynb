{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish CNNs from scratch\n",
    "Activation function layers still contain two proccesses, namely **Forward Propogation** and **Backward Propogation**. They are set for multiple purposes, like Non-linearity, Thresholding / Decision making, Squashing / Bounding values, Differentiability, Mitigate vanishing and exploding gradient problems, computational efficiency, and representation and learning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from layer import Layer\n",
    "from activation import Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Non-Linearity\n",
    "The reason for introducing the activation layer, which often uses a non-linear activation function, is that without these non-linear functions (or in other words, with only linear activation functions), multiple layers would eventually become just one layer. Here is the mathmatical provement.  \n",
    "Suppose that we employ two layers with a linear activation function:\n",
    "$$\n",
    "y_{1}=\\omega_{1} x_{1}+b_{1}\n",
    "$$\n",
    "$$\n",
    "\\phi_{activation} (y_{1})=y_{1}\n",
    "$$\n",
    "$$\n",
    "y_{2}=\\omega_{2} x_{2}+b_{2} \n",
    "$$\n",
    "where $x_{2}=\\phi_{activation} (y_{1})$  \n",
    "Hence\n",
    "$$\n",
    "y_{2}=\\omega_{1} \\omega_{2} x_{1}+\\omega_{2} b_{1}+b_{2}= \\omega_{3}  x_{1}+b_{3}\n",
    "$$\n",
    "where $\\omega_{3}=\\omega_{1} \\omega_{2} , b_{3}=\\omega_{2} b_{1}+b_{2}$   \n",
    "The hidden layer is useless with the existence of the linear activation fucntion.  \n",
    "\n",
    "### 2.Commonly used activation functions\n",
    "Some commonly used non-linear activation functions include Tanh, Sigmoid, ReLu, etc.   \n",
    "$$\n",
    "\\tanh(x)=\\frac{e^{x}-e^{x}}{e^{x}+e^{x}}   \n",
    "$$\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}} \n",
    "$$\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)   \n",
    "$$   \n",
    "And their corresponding derivatives go as follows:\n",
    "$$\n",
    "\\tanh'(x) = 1 - \\tanh^2(x) \n",
    "$$\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x)) \n",
    "$$\n",
    "$$\n",
    "\\text{ReLU}'(x) = \n",
    "\\begin{cases} \n",
    "1 & \\text{if } x > 0 \\\\\n",
    "0 & \\text{if } x < 0 \\\\\n",
    "\\text{user-defined} & \\text{if } x = 0\n",
    "\\end{cases} \n",
    "$$  \n",
    "The formula of **Forward Propogation** goes as follows: \n",
    "$$\n",
    "Y=f(X) \\tag{1}\n",
    "$$ \n",
    "The formula of **Backward Propogation** goes as follows:\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial X} = \\frac{\\partial E}{\\partial Y} \\odot f'(X) \\tag{2}\n",
    "$$  \n",
    "where $\\odot$ denotes the element-wise multiplication of two matrixs.\n",
    "\n",
    "### 3.Softmax activation layer\n",
    "In CNNs, softmax activation function layer is the last layer of the whole model, following the fully connected layer.\n",
    "\n",
    "The formula of **Forward Propogation**, which generates the predicted probilities, goes as follows:\n",
    "$$\n",
    "\\hat{y_{i}}=\\frac{e^{x_{i}}}{\\sum_{n}^{j=1}e^{x_{j}}} \\tag{3} \n",
    "$$  \n",
    "The formula of **Backward Propogation**, whose input ($\\partial_{Y}E$) is from the derivatives of cross-entropy loss with respect to predicted outcomes,  goes as follows:\n",
    "$$\n",
    "\\partial_{X} E =(M \\odot (I-M^T)) \\cdot \\partial_{Y}E  \\tag{4} \n",
    "$$\n",
    "$$\n",
    "M=\n",
    "\\begin{bmatrix}\n",
    "y_{1} & y_{1} & \\dots & y_{1}\\\\\n",
    "y_{2} & y_{2} & \\dots & y_{2}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "y_{n} & y_{n} & \\dots & y_{n}\\\\\n",
    "\\end{bmatrix}\n",
    "\\;\n",
    "\\partial_{X} E =\n",
    "\\begin{bmatrix}\n",
    "\\partial_{x_{1}} E\\\\\n",
    "\\partial_{x_{2}} E\\\\\n",
    "\\vdots\\\\\n",
    "\\partial_{x_{n}} E\\\\\n",
    "\\end{bmatrix}\n",
    "\\partial_{Y} E =\n",
    "\\begin{bmatrix}\n",
    "\\partial_{y_{1}} E\\\\\n",
    "\\partial_{y_{2}} E\\\\\n",
    "\\vdots\\\\\n",
    "\\partial_{y_{n}} E\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where $\\odot$ denotes the element-wise multiplication of two matrixs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        def tanh(x):\n",
    "            return np.tanh(x)\n",
    "\n",
    "        def tanh_prime(x):\n",
    "            return 1 - np.tanh(x) ** 2\n",
    "\n",
    "        super().__init__(tanh, tanh_prime)\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-np.clip(x,-100,100)))\n",
    "\n",
    "        def sigmoid_prime(x):\n",
    "            s = sigmoid(x)\n",
    "            return s * (1 - s)\n",
    "\n",
    "        super().__init__(sigmoid, sigmoid_prime)\n",
    "\n",
    "class ReLu(Activation):\n",
    "    def __init__(self):\n",
    "        def relu(x):\n",
    "            return np.maximum(0, x)\n",
    "        def relu_prime(x):\n",
    "            return np.where(x > 0, 1, 0)\n",
    "        \n",
    "        super().__init__(relu,relu_prime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supplementry information from Chatgpt  \n",
    "In neural networks, activation functions, which are often represented by activation layers, play a crucial role. Here's a breakdown of their purposes:\n",
    "\n",
    "1. **Non-linearity**: The most essential role of an activation function is to introduce non-linearity into the network. Without non-linearity, even a deep neural network would behave just like a single-layer linear model because the composition of linear functions remains linear. Non-linearity allows the network to capture and model more complex relationships in the data.\n",
    "\n",
    "2. **Thresholding / Decision Making**: Activation functions like the step function (rarely used in practice for hidden layers) or the ReLU (Rectified Linear Unit) can be thought of as making decisions – they determine if a particular neuron should be activated or not based on the weighted sum of its inputs.\n",
    "\n",
    "3. **Squashing / Bounding Values**: Some activation functions, like the sigmoid or tanh, squash the incoming values into a specific range. For instance, the sigmoid function maps values to the range (0,1), while tanh maps to the range (-1,1). This can be useful in ensuring that the neuron’s output doesn't reach extremely high or low values, and in some contexts, like output layers for binary classification, a bounded output is desired.\n",
    "\n",
    "4. **Differentiability**: Many optimization methods, such as gradient descent, rely on the ability to compute gradients or derivatives. For this reason, activation functions used in practice (like sigmoid, tanh, ReLU, etc.) are often differentiable (with some minor exceptions, like the exact point at 0 for ReLU, but workarounds exist).\n",
    "\n",
    "5. **Mitigate Vanishing and Exploding Gradient Problems**: Certain activation functions can help in mitigating the issues of vanishing or exploding gradients, which are common in deep networks. For example, the ReLU function doesn't squash values, making it less prone to the vanishing gradient problem than the sigmoid or tanh, especially in deep networks. However, ReLU can suffer from exploding gradients, which is why there are variants like Leaky ReLU and Parametric ReLU.\n",
    "\n",
    "6. **Computational Efficiency**: Some activation functions are computationally more efficient to compute than others. For instance, ReLU and its variants tend to be faster than sigmoid or tanh since they involve simpler mathematical operations.\n",
    "\n",
    "7. **Representation and Learning Capabilities**: Different activation functions can lead to different learning dynamics and capabilities. Some may help networks converge faster, while others may help in capturing more nuanced patterns.\n",
    "\n",
    "In summary, activation functions help neural networks model complex, non-linear relationships, ensure computational stability, and offer desired properties for optimization and learning dynamics. The choice of activation function can significantly impact the performance and training dynamics of a neural network, and it's often a subject of empirical study and research in deep learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
